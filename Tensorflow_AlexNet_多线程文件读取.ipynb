{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "tf.reset_default_graph()\n",
    "data_path=\"D:\\\\log\\\\kaggle\\\\PetImages\"\n",
    "tf_data_path=\"D:\\\\log\\\\kaggle\\\\TFRecordData\"\n",
    "def CreateTFRecordData(data_path,tf_data_path):\n",
    "    print(\"Create TFRecord:\")\n",
    "    if not os.path.exists(tf_data_path):\n",
    "        os.makedirs(tf_data_path)\n",
    "    if not os.path.exists(os.path.join(tf_data_path,\"train_data\")):\n",
    "        os.makedirs(os.path.join(tf_data_path,\"train_data\"))\n",
    "    if not os.path.exists(os.path.join(tf_data_path,\"test_data\")):\n",
    "        os.makedirs(os.path.join(tf_data_path,\"test_data\"))\n",
    "    train_writer=tf.python_io.TFRecordWriter(os.path.join(tf_data_path,\"train_data\\\\train.TFRecord\"))\n",
    "    test_writer=tf.python_io.TFRecordWriter(os.path.join(tf_data_path,\"test_data\\\\test.TFRecords\"))\n",
    "    classes=os.listdir(data_path)\n",
    "    image_train_path=[]\n",
    "    image_train_label=[]\n",
    "    image_test_path=[]\n",
    "    image_test_label=[]\n",
    "    for label,class_name in enumerate(classes):\n",
    "        class_path=os.path.join(data_path,class_name)\n",
    "        image_index=0\n",
    "        for image_name in os.listdir(class_path):\n",
    "            image_index+=1\n",
    "            image_path=os.path.join(class_path,image_name)\n",
    "            if image_index<=12000:\n",
    "                image_train_path.append(image_path)\n",
    "                image_train_label.append(label)\n",
    "            else:\n",
    "                image_test_path.append(image_path)\n",
    "                image_test_label.append(label)\n",
    "    image_train_path,image_train_label=shuffle(image_train_path,image_train_label)\n",
    "    image_test_path,image_test_label=shuffle(image_test_path,image_test_label)\n",
    "    image_train_path,image_train_label=shuffle(image_train_path,image_train_label)\n",
    "    image_test_path,image_test_label=shuffle(image_test_path,image_test_label)\n",
    "    for i in range(len(image_train_path)):\n",
    "        if (i+1)%2000==0:\n",
    "            print(\"creating train_data TFRecord file:%.3f%%\"%(100*i/len(image_train_path)))\n",
    "        image=Image.open(image_train_path[i]).resize((150,150))\n",
    "        image=image.convert(\"RGB\").tobytes()\n",
    "        example=tf.train.Example(features=tf.train.Features(feature={\n",
    "                    \"image\":tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n",
    "                    \"label\":tf.train.Feature(int64_list=tf.train.Int64List(value=[image_train_label[i]]))\n",
    "                }))\n",
    "        train_writer.write(example.SerializeToString())\n",
    "    for i in range(len(image_test_path)):\n",
    "        if (i+1)%200==0:\n",
    "            print(\"creating train_data TFRecord file:%.3f%%\"%(100*i/len(image_test_path)))\n",
    "        image=Image.open(image_test_path[i]).resize((150,150))\n",
    "        image=image.convert(\"RGB\").tobytes()\n",
    "        example=tf.train.Example(features=tf.train.Features(feature={\n",
    "                    \"image\":tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n",
    "                    \"label\":tf.train.Feature(int64_list=tf.train.Int64List(value=[image_test_label[i]]))\n",
    "                }))\n",
    "        test_writer.write(example.SerializeToString())\n",
    "    train_writer.close()\n",
    "    test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find data file: D:\\log\\kaggle\\TFRecordData\\train_data\\train.TFRecord\n",
      "find data file: D:\\log\\kaggle\\TFRecordData\\test_data\\test.TFRecords\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "tf.reset_default_graph()\n",
    "data_path=\"D:\\\\log\\\\kaggle\\\\PetImages\"\n",
    "tf_data_path=\"D:\\\\log\\\\kaggle\\\\TFRecordData\"\n",
    "trained_model_path='D:\\\\log\\\\kaggle\\\\trained_model_path_AlexNet_kaggle'\n",
    "name=[\"cat\",\"dog\"]\n",
    "def imshows(images,labels,index,amount,predictions=None):\n",
    "    fig=plt.gcf()\n",
    "    fig.set_size_inches(10,20)\n",
    "    for i in range(amount):\n",
    "        title=\"lab:\"+name[np.argmax(labels[index+i])]\n",
    "        if predictions is not None:\n",
    "            title=title+\"prd:\"+name[np.argmax(predictions[index+i])]\n",
    "        ax=plt.subplot(5,6,i+1)\n",
    "        ax.set_title(title)\n",
    "        ax.imshow(images[index+i])\n",
    "    plt.show()\n",
    "def Batcher(name,batch_size,enhance=False,path=tf_data_path,min_after_dequeue=1500,num_threads=2):\n",
    "    path=os.path.join(tf_data_path,name)\n",
    "    filename=os.listdir(os.path.join(tf_data_path,name))\n",
    "    for i in range(len(filename)):\n",
    "        filename[i]=os.path.join(path,filename[i])\n",
    "        print(\"find data file:\",filename[i])\n",
    "    filename_queue=tf.train.string_input_producer(filename,shuffle=False,num_epochs=None)\n",
    "    reader=tf.TFRecordReader()\n",
    "    _,serialized_examples=reader.read(filename_queue)\n",
    "    features=tf.parse_single_example(serialized_examples,features={\n",
    "            \"image\":tf.FixedLenFeature([],tf.string),\n",
    "            \"label\":tf.FixedLenFeature([],tf.int64)\n",
    "        })\n",
    "    image=tf.decode_raw(features[\"image\"],tf.uint8)\n",
    "    label=tf.cast(features[\"label\"],tf.int32)\n",
    "    image=tf.reshape(image,[150,150,3])\n",
    "    image=tf.random_crop(image,[128,128,3])\n",
    "    if enhance:\n",
    "        image=tf.image.random_flip_left_right(image)\n",
    "        image=tf.image.random_flip_up_down(image)\n",
    "        image=tf.image.random_brightness(image,max_delta=0.4)\n",
    "        image=tf.image.random_contrast(image,lower=0.7,upper=1.3)\n",
    "        image=tf.image.random_saturation(image,lower=0.7,upper=1.3)\n",
    "    image=tf.image.per_image_standardization(image)\n",
    "    images,labels=tf.train.shuffle_batch([image,label],\n",
    "                                         num_threads=num_threads,\n",
    "                                         batch_size=batch_size,\n",
    "                                         min_after_dequeue=min_after_dequeue,\n",
    "                                         capacity=min_after_dequeue+batch_size*3)\n",
    "    return images,tf.one_hot(labels,2)\n",
    "#CreateTFRecordData(data_path,tf_data_path)\n",
    "image_train,label_train=Batcher(name=\"train_data\",batch_size=256,enhance=True)\n",
    "image_test,label_test=Batcher(name=\"test_data\",batch_size=256,enhance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exponential_decay_with_warmup(warmup_step,learning_rate_base,global_step,learning_rate_step,learning_rate_decay,staircase=False):\n",
    "    warmup_step=tf.constant(warmup_step)\n",
    "    linear_increase=learning_rate_base*tf.cast(global_step/warmup_step,tf.float32)\n",
    "    exponential_decay=tf.train.exponential_decay(learning_rate_base,\n",
    "                                                                  global_step-warmup_step,\n",
    "                                                                  learning_rate_step,\n",
    "                                                                  learning_rate_decay,\n",
    "                                                                  staircase=staircase)\n",
    "    learning_rate=tf.cond(global_step<=warmup_step,lambda:linear_increase,lambda:exponential_decay)\n",
    "    return learning_rate\n",
    "class CNN():\n",
    "    def __init__(self,drop_rate=0.0,is_training=False,regularizer=None,regularizer_name='losses',average_class=None):\n",
    "        self.layer_index=1\n",
    "        self.drop_rate=drop_rate\n",
    "        self.is_training=is_training\n",
    "        self.Rname=regularizer_name\n",
    "        if regularizer is None:\n",
    "            self.regularizer=None\n",
    "        else:\n",
    "            self.regularizer=regularizer\n",
    "        if average_class is None:\n",
    "            self.ema=None\n",
    "        else:\n",
    "            self.ema=average_class.average\n",
    "    def Xavier(self): #initializer\n",
    "        return tf.contrib.layers.xavier_initializer()\n",
    "    def MSRA(self,shape): # initializer\n",
    "        if len(shape)==2:\n",
    "            return tf.truncated_normal_initializer(stddev=2/shape[1])\n",
    "        else:\n",
    "            return tf.truncated_normal_initializer(stddev=2/(shape[0]*shape[1]*shape[2]))\n",
    "    def Sigmoid(self,tensor): #activation\n",
    "        with tf.variable_scope(\"L%d_Sigmoid\"%self.layer_index,reuse=tf.AUTO_REUSE):\n",
    "            self.layer_index+=1\n",
    "            return tf.nn.sigmoid(tensor)   \n",
    "    def Relu(self,tensor):  #activation\n",
    "        with tf.variable_scope(\"L%d_Relu\"%self.layer_index,reuse=tf.AUTO_REUSE):\n",
    "            self.layer_index+=1\n",
    "            return tf.nn.relu(tensor)\n",
    "    def Conv2d(self,tensor,output_channel,ksize,strides,padding=\"VALID\"):\n",
    "        with tf.variable_scope(\"L%d_Conv2d\"%self.layer_index,reuse=tf.AUTO_REUSE):\n",
    "            self.layer_index+=1\n",
    "            input_channel=tensor.shape.as_list()[-1]\n",
    "            kernel_shape=[ksize,ksize,input_channel,output_channel]\n",
    "            kernel=tf.get_variable(\"kernel\",kernel_shape,initializer=self.Xavier())\n",
    "            bias=tf.get_variable(\"bias\",[output_channel],initializer=tf.constant_initializer(0.0))\n",
    "            if self.ema is not None: \n",
    "                conv2d=tf.nn.conv2d(tensor,self.ema(kernel),strides=[1,strides,strides,1],padding=padding)\n",
    "                bias_add=tf.nn.bias_add(conv2d,self.ema(bias))\n",
    "            else:\n",
    "                conv2d=tf.nn.conv2d(tensor,kernel,strides=[1,strides,strides,1],padding=padding)\n",
    "                bias_add=tf.nn.bias_add(conv2d,bias)\n",
    "            return bias_add\n",
    "    def MaxPooling2d(self,tensor,ksize,strides,padding=\"VALID\"):\n",
    "        with tf.variable_scope(\"L%d_MaxPooling2d\"%self.layer_index,reuse=tf.AUTO_REUSE):\n",
    "            self.layer_index+=1\n",
    "            return tf.nn.max_pool(tensor,ksize=[1,ksize,ksize,1],strides=[1,strides,strides,1],padding=padding)\n",
    "    def BatchNormalization(self,tensor):\n",
    "        with tf.variable_scope(\"L%d_BatchNorm\"%self.layer_index,reuse=tf.AUTO_REUSE):\n",
    "            self.layer_index+=1\n",
    "            return tf.layers.batch_normalization(tensor,training=self.is_training)\n",
    "    def Dropout(self,tensor,drop_rate):\n",
    "        with tf.variable_scope(\"L%d_Dropout\"%self.layer_index,reuse=tf.AUTO_REUSE):\n",
    "            self.layer_index+=1\n",
    "        return tf.layers.dropout(tensor,drop_rate,training=self.is_training)\n",
    "    def GlobalAvgPooling2d(self,tensor):\n",
    "        with tf.variable_scope(\"L%d_GlobalAvgPooling2d\"%self.layer_index,reuse=tf.AUTO_REUSE):\n",
    "            self.layer_index+=1\n",
    "            tensor_shape=tensor.shape.as_list()\n",
    "            ksize=[1,tensor_shape[1],tensor_shape[2],1]\n",
    "            return tf.nn.avg_pool(tensor,ksize=ksize,strides=[1,1,1,1],padding=\"VALID\")\n",
    "    def FC_conv2d(self,tensor,output_channel):  #1x1卷积代替全连接层\n",
    "        with tf.variable_scope(\"L%d_FullCon\"%self.layer_index,reuse=tf.AUTO_REUSE):\n",
    "            self.layer_index+=1\n",
    "            return self.Conv2d(tensor,output_channel=output_channel,ksize=1,strides=1,padding=\"SAME\")\n",
    "    def SE_Block(self,tensor,SE_rate=16): #Sequeeze and Excitation Block,or SENet,which is a subNet\n",
    "         with tf.variable_scope(\"L%d_SqueezeAndExcitation\"%self.layer_index,reuse=tf.AUTO_REUSE):\n",
    "            self.layer_index+=1       \n",
    "            input_channel=tensor.shape.as_list()[-1]\n",
    "            squeeze=self.GlobalAvgPooling2d(tensor)\n",
    "            excitation=self.FC_conv2d(squeeze,input_channel/SE_rate)\n",
    "            BN=self.BatchNormalization(excitation)\n",
    "            relu=self.Relu(BN)\n",
    "            excitation=self.FC_conv2d(relu,input_channel)\n",
    "            sigmoid=self.Sigmoid(excitation)\n",
    "            return sigmoid*tensor\n",
    "    def AlexNet(self,tensor):\n",
    "        conv=self.Conv2d(tensor,output_channel=96,ksize=11,strides=4,padding=\"VALID\")\n",
    "        BN=self.BatchNormalization(conv)\n",
    "        relu=self.Relu(BN)\n",
    "        \n",
    "        SE=self.SE_Block(relu)\n",
    "        \n",
    "        pool=self.MaxPooling2d(SE,ksize=3,strides=2,padding=\"VALID\")\n",
    "        \n",
    "        conv=self.Conv2d(pool,output_channel=256,ksize=5,strides=2,padding=\"SAME\")\n",
    "        BN=self.BatchNormalization(conv)\n",
    "        relu=self.Relu(BN)\n",
    "        \n",
    "        SE=self.SE_Block(relu)\n",
    "        \n",
    "        \n",
    "        conv=self.Conv2d(SE,output_channel=384,ksize=3,strides=1,padding=\"SAME\")\n",
    "        BN=self.BatchNormalization(conv)\n",
    "        relu=self.Relu(BN)\n",
    "        \n",
    "        SE=self.SE_Block(relu)\n",
    "        \n",
    "        conv=self.Conv2d(SE,output_channel=384,ksize=3,strides=1,padding=\"SAME\")\n",
    "        BN=self.BatchNormalization(conv)\n",
    "        relu=self.Relu(BN)\n",
    "        \n",
    "        SE=self.SE_Block(relu)\n",
    "        \n",
    "        conv=self.Conv2d(SE,output_channel=256,ksize=3,strides=2,padding=\"SAME\")\n",
    "        BN=self.BatchNormalization(conv)\n",
    "        relu=self.Relu(BN)\n",
    "        SE=self.SE_Block(relu)\n",
    "        \n",
    "        \n",
    "        Global_avg_pool=self.GlobalAvgPooling2d(SE)\n",
    "        BN=self.BatchNormalization(Global_avg_pool)\n",
    "        \n",
    "        full=self.FC_conv2d(BN,784)\n",
    "        BN=self.BatchNormalization(full)\n",
    "        relu=self.Relu(BN)\n",
    "        dropout=self.Dropout(relu,drop_rate=self.drop_rate)\n",
    "        \n",
    "        full=self.FC_conv2d(BN,128)\n",
    "        BN=self.BatchNormalization(full)\n",
    "        relu=self.Relu(BN)\n",
    "        dropout=self.Dropout(relu,drop_rate=self.drop_rate)\n",
    "        \n",
    "        full=self.FC_conv2d(dropout,2)\n",
    "        return tf.reshape(full,[-1,2])\n",
    "        \n",
    "x=tf.placeholder(tf.float32,[None,128,128,3])\n",
    "y=tf.placeholder(tf.float32,[None,2])\n",
    "is_training=tf.placeholder(tf.bool)\n",
    "train_epoches=60\n",
    "batch_size=150\n",
    "batch_num=int(24000/batch_size)\n",
    "#预热的指数衰减学习率\n",
    "global_step=tf.Variable(0,trainable=False)\n",
    "warmup_step=batch_num*3\n",
    "learning_rate_base=0.015\n",
    "learning_rate_decay=0.95\n",
    "learning_rate_step=batch_num\n",
    "learning_rate=exponential_decay_with_warmup(warmup_step,learning_rate_base,global_step,learning_rate_step,learning_rate_decay)\n",
    "#正则化和前向预测\n",
    "regularizer=tf.contrib.layers.l2_regularizer(0.008)\n",
    "forward=CNN(drop_rate=0.5,is_training=is_training,regularizer=regularizer,average_class=None).AlexNet(x)\n",
    "#滑动平均\n",
    "moving_average_decay=0.999\n",
    "ema=tf.train.ExponentialMovingAverage(moving_average_decay,global_step)\n",
    "ema_op=ema.apply(tf.trainable_variables())\n",
    "ema_forward=CNN(drop_rate=0.2,is_training=is_training,regularizer=None,average_class=ema).AlexNet(x)\n",
    "prediction=tf.nn.softmax(ema_forward)\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "#损失函数和优化器\n",
    "momentum=0.9\n",
    "cross_entropy=tf.losses.softmax_cross_entropy(onehot_labels=y,logits=forward,label_smoothing=0.1)\n",
    "l2_regularizer_loss=tf.add_n(tf.get_collection(\"losses\"))\n",
    "loss_function=cross_entropy+l2_regularizer_loss\n",
    "update_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer=tf.train.MomentumOptimizer(learning_rate,momentum).minimize(loss_function,global_step=global_step)\n",
    "if not os.path.exists(trained_model_path):\n",
    "    os.makedirs(trained_model_path)\n",
    "saver=tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:\\log\\kaggle\\trained_model_path_AlexNet_kaggle\\trained_model_37\n",
      "training start!\n",
      "0 0.53125\n",
      "1 0.484375\n",
      "2 0.48828125\n",
      "3 0.54296875\n",
      "4 0.5234375\n",
      "5 0.51171875\n",
      "6 0.58203125\n",
      "7 0.5703125\n",
      "8 0.58984375\n",
      "9 0.55859375\n",
      "10 0.5859375\n",
      "11 0.5390625\n",
      "12 0.5390625\n",
      "13 0.6171875\n",
      "14 0.609375\n",
      "15 0.55859375\n",
      "16 0.63671875\n",
      "17 0.55859375\n",
      "18 0.53125\n",
      "19 0.5390625\n",
      "20 0.54296875\n",
      "21 0.5234375\n",
      "22 0.57421875\n",
      "23 0.57421875\n",
      "24 0.578125\n",
      "25 0.5625\n",
      "26 0.609375\n",
      "27 0.59375\n",
      "28 0.6015625\n",
      "29 0.6015625\n",
      "30 0.62890625\n",
      "31 0.625\n",
      "32 0.58203125\n",
      "33 0.5703125\n",
      "34 0.61328125\n",
      "35 0.578125\n",
      "36 0.58984375\n",
      "37 0.60546875\n",
      "38 0.58203125\n",
      "39 0.6171875\n",
      "40 0.61328125\n",
      "41 0.6015625\n",
      "42 0.60546875\n",
      "43 0.5625\n",
      "44 0.57421875\n",
      "45 0.60546875\n",
      "46 0.609375\n",
      "47 0.578125\n",
      "48 0.62109375\n",
      "49 0.5625\n",
      "50 0.58203125\n",
      "51 0.62890625\n",
      "52 0.64453125\n",
      "53 0.66796875\n",
      "54 0.6328125\n",
      "55 0.67578125\n",
      "56 0.64453125\n",
      "57 0.671875\n",
      "58 0.64453125\n",
      "59 0.609375\n",
      "60 0.58984375\n",
      "61 0.6015625\n",
      "62 0.62109375\n",
      "63 0.65234375\n",
      "64 0.640625\n",
      "65 0.6328125\n",
      "66 0.6015625\n",
      "67 0.6640625\n",
      "68 0.6328125\n",
      "69 0.58203125\n",
      "70 0.609375\n",
      "71 0.64453125\n",
      "72 0.6328125\n",
      "73 0.64453125\n",
      "74 0.625\n",
      "75 0.609375\n",
      "76 0.58984375\n",
      "77 0.62890625\n",
      "78 0.64453125\n",
      "79 0.55859375\n",
      "80 0.59375\n",
      "81 0.6171875\n",
      "82 0.62890625\n",
      "83 0.64453125\n",
      "84 0.625\n",
      "85 0.61328125\n",
      "86 0.609375\n",
      "87 0.58984375\n",
      "88 0.6328125\n",
      "89 0.61328125\n",
      "90 0.59765625\n",
      "91 0.66015625\n",
      "92 0.59765625\n",
      "93 0.61328125\n",
      "94 0.64453125\n",
      "95 0.671875\n",
      "96 0.6328125\n",
      "97 0.671875\n",
      "98 0.64453125\n",
      "99 0.6484375\n",
      "100 0.63671875\n",
      "101 0.703125\n",
      "102 0.66796875\n",
      "103 0.66796875\n",
      "104 0.67578125\n",
      "105 0.6484375\n",
      "106 0.609375\n",
      "107 0.625\n",
      "108 0.63671875\n",
      "109 0.5859375\n",
      "110 0.6953125\n",
      "111 0.68359375\n",
      "112 0.63671875\n",
      "113 0.64453125\n",
      "114 0.6796875\n",
      "115 0.65625\n",
      "116 0.625\n",
      "117 0.65234375\n",
      "118 0.66015625\n",
      "119 0.671875\n",
      "120 0.625\n",
      "121 0.66796875\n",
      "122 0.66015625\n",
      "123 0.65625\n",
      "124 0.65625\n",
      "125 0.6640625\n",
      "126 0.65625\n",
      "127 0.6328125\n",
      "128 0.6015625\n",
      "129 0.703125\n",
      "130 0.69921875\n",
      "131 0.6328125\n",
      "132 0.70703125\n",
      "133 0.63671875\n",
      "134 0.65625\n",
      "135 0.6953125\n",
      "136 0.65625\n",
      "137 0.65234375\n",
      "138 0.59765625\n",
      "139 0.70703125\n",
      "140 0.703125\n",
      "141 0.71484375\n",
      "142 0.734375\n",
      "143 0.6796875\n",
      "144 0.67578125\n",
      "145 0.6796875\n",
      "146 0.65625\n",
      "147 0.6875\n",
      "148 0.640625\n",
      "149 0.66015625\n",
      "150 0.640625\n",
      "151 0.73828125\n",
      "152 0.64453125\n",
      "153 0.7578125\n",
      "154 0.671875\n",
      "155 0.67578125\n",
      "156 0.75\n",
      "157 0.70703125\n",
      "158 0.66796875\n",
      "159 0.66015625\n",
      "Epoch:1 Speed:636.25 seconds per epoch -> Accuracy:0.730  Loss:1.259\n",
      "learning rate:0.0050000 \n",
      "model_1 saved\n",
      "0 0.6484375\n",
      "1 0.6875\n",
      "2 0.6953125\n",
      "3 0.671875\n",
      "4 0.69921875\n",
      "5 0.7265625\n",
      "6 0.6953125\n",
      "7 0.70703125\n",
      "8 0.71484375\n",
      "9 0.734375\n",
      "10 0.70703125\n",
      "11 0.69140625\n",
      "12 0.70703125\n",
      "13 0.67578125\n",
      "14 0.71484375\n",
      "15 0.63671875\n",
      "16 0.7109375\n",
      "17 0.6796875\n",
      "18 0.6640625\n",
      "19 0.6953125\n",
      "20 0.68359375\n",
      "21 0.64453125\n",
      "22 0.671875\n",
      "23 0.69921875\n",
      "24 0.6953125\n",
      "25 0.76953125\n",
      "26 0.71484375\n",
      "27 0.6875\n",
      "28 0.6953125\n",
      "29 0.6875\n",
      "30 0.70703125\n",
      "31 0.72265625\n",
      "32 0.671875\n",
      "33 0.74609375\n",
      "34 0.6953125\n",
      "35 0.6796875\n",
      "36 0.62109375\n",
      "37 0.70703125\n",
      "38 0.6875\n",
      "39 0.6953125\n",
      "40 0.7109375\n",
      "41 0.6484375\n",
      "42 0.69921875\n",
      "43 0.640625\n",
      "44 0.63671875\n",
      "45 0.6953125\n",
      "46 0.65625\n",
      "47 0.703125\n",
      "48 0.6796875\n",
      "49 0.703125\n",
      "50 0.6796875\n",
      "51 0.6953125\n",
      "52 0.72265625\n",
      "53 0.71875\n",
      "54 0.70703125\n",
      "55 0.671875\n",
      "56 0.70703125\n",
      "57 0.67578125\n",
      "58 0.66796875\n",
      "59 0.69921875\n",
      "60 0.71875\n",
      "61 0.75390625\n",
      "62 0.6875\n",
      "63 0.6796875\n",
      "64 0.73828125\n",
      "65 0.7265625\n",
      "66 0.703125\n",
      "67 0.71484375\n",
      "68 0.66015625\n",
      "69 0.73046875\n",
      "70 0.7265625\n",
      "71 0.7109375\n",
      "72 0.6953125\n",
      "73 0.7421875\n",
      "74 0.71875\n",
      "75 0.73046875\n",
      "76 0.7265625\n",
      "77 0.69140625\n",
      "78 0.71484375\n",
      "79 0.6796875\n",
      "80 0.69921875\n",
      "81 0.703125\n",
      "82 0.71484375\n",
      "83 0.6953125\n",
      "84 0.75\n",
      "85 0.75\n",
      "86 0.67578125\n",
      "87 0.703125\n",
      "88 0.7265625\n",
      "89 0.69140625\n",
      "90 0.69140625\n",
      "91 0.67578125\n",
      "92 0.69140625\n",
      "93 0.7265625\n",
      "94 0.71875\n",
      "95 0.71875\n",
      "96 0.703125\n",
      "97 0.734375\n",
      "98 0.6875\n",
      "99 0.7109375\n",
      "100 0.73046875\n",
      "101 0.7265625\n",
      "102 0.70703125\n",
      "103 0.73046875\n",
      "104 0.734375\n",
      "105 0.73828125\n",
      "106 0.6484375\n",
      "107 0.6640625\n",
      "108 0.72265625\n",
      "109 0.6953125\n",
      "110 0.6953125\n",
      "111 0.73828125\n",
      "112 0.6953125\n",
      "113 0.734375\n",
      "114 0.7109375\n",
      "115 0.73046875\n",
      "116 0.7265625\n",
      "117 0.66796875\n",
      "118 0.66796875\n",
      "119 0.78125\n",
      "120 0.70703125\n",
      "121 0.6953125\n",
      "122 0.7109375\n",
      "123 0.7578125\n",
      "124 0.69921875\n",
      "125 0.71484375\n",
      "126 0.68359375\n",
      "127 0.6484375\n",
      "128 0.73828125\n",
      "129 0.71875\n",
      "130 0.6875\n",
      "131 0.69921875\n",
      "132 0.75\n",
      "133 0.73046875\n",
      "134 0.6953125\n",
      "135 0.671875\n",
      "136 0.73046875\n",
      "137 0.6875\n",
      "138 0.7265625\n",
      "139 0.6953125\n",
      "140 0.734375\n",
      "141 0.71484375\n",
      "142 0.72265625\n",
      "143 0.703125\n",
      "144 0.703125\n",
      "145 0.70703125\n",
      "146 0.73046875\n",
      "147 0.73046875\n",
      "148 0.7265625\n",
      "149 0.72265625\n",
      "150 0.77734375\n",
      "151 0.7421875\n",
      "152 0.6953125\n",
      "153 0.65234375\n",
      "154 0.73046875\n",
      "155 0.68359375\n",
      "156 0.71875\n",
      "157 0.73828125\n",
      "158 0.7421875\n",
      "159 0.703125\n",
      "Epoch:2 Speed:628.61 seconds per epoch -> Accuracy:0.738  Loss:1.882\n",
      "learning rate:0.0100000 \n",
      "model_2 saved\n",
      "0 0.72265625\n",
      "1 0.71484375\n",
      "2 0.73828125\n",
      "3 0.72265625\n",
      "4 0.75\n",
      "5 0.7421875\n",
      "6 0.72265625\n",
      "7 0.76953125\n",
      "8 0.6875\n",
      "9 0.7578125\n",
      "10 0.72265625\n",
      "11 0.765625\n",
      "12 0.71484375\n",
      "13 0.73046875\n"
     ]
    }
   ],
   "source": [
    "#开始进行训练\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "acc_1_list=[0.0]\n",
    "loss_1_list=[0.0]\n",
    "acc=0.0\n",
    "loss=0.0\n",
    "step=0    \n",
    "start_time=time()\n",
    "image_path=\"D:\\\\log\\\\kaggle\\\\4.jpg\"\n",
    "image=Image.open(image_path)\n",
    "std_image=image.resize((128,128))\n",
    "std_image=std_image.convert(\"RGB\")\n",
    "std_image=tf.image.per_image_standardization(std_image)\n",
    "std_image=tf.reshape(std_image,[1,128,128,3])\n",
    "with tf.Session() as sess:\n",
    "    trained_model_state=tf.train.get_checkpoint_state(trained_model_path)\n",
    "    if trained_model_state and trained_model_state.model_checkpoint_path:\n",
    "        saver.restore(sess,trained_model_state.model_checkpoint_path)    \n",
    "    sess.run([tf.local_variables_initializer(),tf.global_variables_initializer()])\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads=tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    print(\"training start!\")\n",
    "    for epoch in range(0,train_epoches):\n",
    "        for batch in range(batch_num):\n",
    "            \n",
    "            images,labels=sess.run([image_train,label_train])\n",
    "            sess.run([optimizer,ema_op],feed_dict={x:images,y:labels,is_training:True})\n",
    "            print(batch,sess.run(accuracy,feed_dict={x:images,y:labels,is_training:True}))\n",
    "            \n",
    "        images,labels=sess.run([image_test,label_test])\n",
    "        loss,acc=sess.run([loss_function,accuracy],feed_dict={x:images,y:labels,is_training:False})\n",
    "        decay=min(0.9,(1+step)/(10+step))\n",
    "        print(\"Epoch:%d Speed:%.2f seconds per epoch -> Accuracy:%.3f  Loss:%.3f\"%(epoch+1,(time()-start_time)/(1+epoch),acc,loss))\n",
    "        print(\"learning rate:%.7f \"%(sess.run(learning_rate)))\n",
    "        saver.save(sess,os.path.join(trained_model_path,\"trained_model_%d\"%(epoch+1)))\n",
    "        print(\"model_%d saved\"%(epoch+1))\n",
    "#        input_image=sess.run(std_image)\n",
    "#        pred=sess.run(prediction,feed_dict={x:input_image,is_training:False})\n",
    "#        plt.imshow(image)\n",
    "#        p=100*pred[0][1]\n",
    "#        plt.title(\"The probability of a dog is:%.2f%%\"%(p))\n",
    "#        plt.show()\n",
    "        \n",
    "    print(\"training finished after %.2f seconds\"%(time()-start_time))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Step')\n",
    "plt.ylabel('Average_Accuracy')\n",
    "plt.plot(acc_1_list[20:])\n",
    "plt.legend(['SE-AlexNet'],loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Average_Loss')\n",
    "plt.plot(loss_1_list[20:])\n",
    "plt.legend(['SE-AlexNet'],loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "image_path=\"D:\\\\log\\\\kaggle\\\\3.jpg\"\n",
    "#image_path=\"D:\\\\log\\\\kaggle\\\\PetImages\\\\Dog\\\\4.jpg\"\n",
    "image=Image.open(image_path)\n",
    "std_image=image.resize((180,180))\n",
    "std_image=std_image.convert(\"RGB\")\n",
    "std_image=tf.random_crop(std_image,[128,128,3])\n",
    "std_image=tf.image.per_image_standardization(std_image)\n",
    "std_image=tf.reshape(std_image,[1,128,128,3])\n",
    "print(\"waiting\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    trained_model_state=tf.train.get_checkpoint_state(trained_model_path)\n",
    "    if trained_model_state and trained_model_state.model_checkpoint_path:\n",
    "        saver.restore(sess,trained_model_state.model_checkpoint_path)\n",
    "    input_image=sess.run(std_image)\n",
    "    pred=sess.run(prediction,feed_dict={x:input_image,is_training:False})\n",
    "    for i in range(9):\n",
    "        pred+=sess.run(prediction,feed_dict={x:input_image,is_training:False})\n",
    "    pred=sess.run(tf.nn.softmax(pred))\n",
    "    plt.imshow(image)\n",
    "    if pred[0][0]>pred[0][1]:\n",
    "        plt.title(\"The probability of a cat is:%.2f%%\"%(100*pred[0][0]))\n",
    "    else:\n",
    "        plt.title(\"The probability of a dog is:%.2f%%\"%(100*pred[0][1]))\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([tf.local_variables_initializer(),tf.global_variables_initializer()])\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads=tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    img,lab=sess.run([image_test,label_test])\n",
    "    imshows(img,lab,0,30)\n",
    "    print(lab)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda]",
   "language": "python",
   "name": "conda-env-Anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
